{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ttycp3/Classdesign/blob/main/NLEQuestion1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#G5119: Natural Language Engineering\n",
        "\n",
        "##Computer Based Examination, 2023\n",
        "\n",
        "Remember, you can add cells and change their type (between code and text/markdown) as required to answer the questions."
      ],
      "metadata": {
        "id": "_M3r84BT5WeN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibHDq-Im5Re4"
      },
      "outputs": [],
      "source": [
        "# update your candidate number here\n",
        "candidate_number = 625937"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 1 (50 Marks)\n",
        "\n",
        "This question is about document similarity and information retrieval."
      ],
      "metadata": {
        "id": "259FEqEb5xRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### do not change the code in this cell\n",
        "# make sure you run this cell\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from math import log2, sqrt\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download(\"punkt\")\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "\n",
        "\n",
        "# This is corpus of quotations, their sources and languages which we will use in this question.\n",
        "corpus = [(\"All the worldâ€™s a stage, and all the men and women merely players.\", \"William Shakespeare\", \"English\"),\n",
        "          (\"Ask not what your country can do for you; ask what you can do for your country.\", \"John Kennedy\", \"English\"),\n",
        "          (\"Ask, and it shall be given you; seek, and you shall find.\", \"the Bible\", \"Greek\"),\n",
        "          (\"Eighty percent of success is showing up.\", \"Woody Allen\", \"English\"),\n",
        "          (\"Elementary, my dear Watson.\", \"Sherlock Holmes (character)\", \"English\"),\n",
        "          (\"For those to whom much is given, much is required.\", \"the Bible\", \"Greek\"),\n",
        "          (\"Frankly, my dear, I don't give a damn.\", \"Rhett Butler (character)\", \"English\"),\n",
        "          (\"Genius is one percent inspiration and ninety-nine percent perspiration.\", \"Thomas Edison\", \"English\")]\n",
        "\n",
        "# This is a query that we will retrieve relevant documents for.\n",
        "query = \"Will I be given what I ask?\""
      ],
      "metadata": {
        "id": "89Wx4WPa5weU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccdbc10f-1bc5-4c94-e9a1-a61a0153f25b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "a) Preprocess the information in the list `corpus` by following the steps below."
      ],
      "metadata": {
        "id": "Ee37cbR8ebN9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "i) Create three dictionaries - `quotes`, `sources` and `languages` - corresponding to the three elements in each tuple of `corpus`. Each key in these dictionaries should be the position in the original list and each value should be a string from the tuple at that position in the list.\n",
        "\n",
        "Each value in `quotes` should be item 0 in a tuple, i.e. the quotation itself. Each value in `sources` should be item 1 in a tuple, i.e. the source associated with the quotation. Each value in `languages` should be item 2 in a tuple, i.e. the original language of the quotation.\n",
        "\n",
        "So for example, the corpus `[(\"Hello\", \"Alice\", \"English\")]` should be broken into three dictionaries: `{0: \"Hello\"}`, `{0: \"Alice\"}` and `{0: \"English\"}`.\n",
        "\n",
        "(7 marks)"
      ],
      "metadata": {
        "id": "GGruck7ketTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quotes = {}\n",
        "sources = {}\n",
        "languages = {}\n",
        "for i, item in enumerate(corpus):\n",
        "    quotes[i] = item[0]\n",
        "    sources[i] = item[1]\n",
        "    languages[i] = item[2]"
      ],
      "metadata": {
        "id": "Lf-jmySgFZAP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ii) Tokenise the quotation strings in the dictionary `quotes` to produce a new dictionary with the same keys called `tokenised`, in which each value is a list of tokens.\n",
        "\n",
        "So, for example the dictionary `{0: \"Hello Bob\"}` would become `{0: [\"Hello\", \"Bob\"]}`.\n",
        "\n",
        "(4 marks)"
      ],
      "metadata": {
        "id": "-HKbt1Npi6U_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenised = {}\n",
        "for i in quotes:\n",
        "    tokenised[i] = word_tokenize(quotes[i].lower())"
      ],
      "metadata": {
        "id": "VpJk-SrsGXSn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "iii) Case normalise the tokenised strings and remove stopwords and punctuation, putting the results into a new dictionary called `normalised`, with the same keys as `tokenised` and the values being normalised lists of tokens.\n",
        "\n",
        "(6 marks)"
      ],
      "metadata": {
        "id": "jMa4g0kGjOze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalised = {}\n",
        "for i in tokenised:\n",
        "    normalised[i] = [word for word in tokenised[i] if word.lower() not in stop and word.isalnum()]"
      ],
      "metadata": {
        "id": "HZVk2RcMGQNY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "iv) Describe two other forms of pre-processing that could be applied to text documents.\n",
        "\n",
        "(4 marks)"
      ],
      "metadata": {
        "id": "TFcNSwE-mmdt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Stemming/Lemmatization: Stemming is the process of reducing words to their base or root form, often by removing inflectional endings. For example, \"running\", \"runs\", and \"runner\" can be reduced to the stem \"run\". Lemmatization is a more sophisticated version that reduces words to their dictionary forms (lemmas), which are actual words instead of just stems. This helps in normalizing text data and grouping together words with similar meanings.\n",
        "\n",
        "2.Part-of-Speech Tagging (POS) and Dependency Parsing: These advanced pre-processing techniques label each word with its grammatical role (POS tagging) or identify the syntactic relationships between words (dependency parsing). This can be useful for understanding the context and structure of sentences before feeding them into machine learning models. The NLTK library provides tools for these tasks too."
      ],
      "metadata": {
        "id": "m9iw4TowmvAQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "i) Calculate document frequencies for each token found in the documents contained in `normalised` and put the results in a dictionary called `df`. That is count how many different entries in the dictionary each token is found in.\n",
        "\n",
        "(4 marks)"
      ],
      "metadata": {
        "id": "c49faFMRdG0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ":b) Convert each document, ie each list of tokens, into a tfidf representation by following the steps below."
      ],
      "metadata": {
        "id": "xzmBCqijjpj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = {}\n",
        "for i, tokens in normalised.items():\n",
        "    for token in set(tokens):\n",
        "        df.setdefault(token, 0)\n",
        "        df[token] += 1\n"
      ],
      "metadata": {
        "id": "Wo96NEz6Hcz8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ii) Calculate inverse document frequencies from the document frequencies derived in the previous question and put the results in a dictionary called `idf`.\n",
        "\n",
        "$$IDF(w) = \\log_2 \\left( \\frac{N}{DF(w)} \\right) $$\n",
        "\n",
        "where $N$ is the total number of documents and $DF(w)$ is the document frequency of the word $w$.\n",
        "\n",
        "(4 marks)"
      ],
      "metadata": {
        "id": "67kNG26edztz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = len(normalised)\n",
        "idf = {}\n",
        "for token in df:\n",
        "    idf[token] = log2(N / df[token])"
      ],
      "metadata": {
        "id": "wqTvv8c_wK0T"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "iii) Convert each document in `normalised` from a list of tokens to a dictionary of term frequencies and put the results in a dictionary called `tf`.\n",
        "\n",
        "The keys of `tf` should be positions in the original list `corpus` and the values should be the term frequency dictionaries, which map from tokens to frequency in each document.\n",
        "\n",
        "So, for example, `{0: [\"Hello\", \"Bob\"]}` would become `{0: {\"Hello\": 1, \"Bob\": 1}}`.\n",
        "\n",
        "(4 marks)"
      ],
      "metadata": {
        "id": "kbTHONWreDTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf = {}\n",
        "for i, tokens in normalised.items():\n",
        "    tf[i] = {token: tokens.count(token) for token in set(tokens)}"
      ],
      "metadata": {
        "id": "M5cfBQOTw7i_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "iv) Convert the raw term frequencies in `tf` to tfidf values using the dictionary `idf`.\n",
        "\n",
        "(3 marks)"
      ],
      "metadata": {
        "id": "kSvH-9m9fWdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = {}\n",
        "for i, terms in tf.items():\n",
        "    tfidf[i] = {token: tf[i][token] * idf[token] for token in terms}"
      ],
      "metadata": {
        "id": "WPL9avGW5rOU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c) In the following steps, preprocess the string `query` and convert it to a tfidf representation then use this to find relevant quotations in the index `tfidf`."
      ],
      "metadata": {
        "id": "BKSzAq1zfj-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "i) Define a function `dot` which takes two dictionaries containing tfidf values as inputs and calculates their dot product.\n",
        "\n",
        "(3 marks)"
      ],
      "metadata": {
        "id": "3GF8dFWsHWlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dot(d1, d2):\n",
        "    sum = 0.0\n",
        "    for token in d1:\n",
        "        if token in d2:\n",
        "            sum += d1[token] * d2[token]\n",
        "    return sum\n"
      ],
      "metadata": {
        "id": "ChDKHmrJFAiz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ii) Define a function, `sim`, which takes two dictionaries containing tfidf values as inputs and , using your `dot` function, calculates their cosine similarity.\n",
        "\n",
        "(3 marks)"
      ],
      "metadata": {
        "id": "vS6SEhUEHpsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sim(d1, d2):\n",
        "    return dot(d1, d2) / sqrt(dot(d1, d1) * dot(d2, d2))\n"
      ],
      "metadata": {
        "id": "9kL3ufDkFOuX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "iii) Preprocess the string `query` and convert it to a tfidf representation. Then calculate its cosine similarity to all the documents in the dictionary `tfidf`.\n",
        "\n",
        "For any document with a non-zero similarity, print out the similarity, source and language of the original quotation.\n",
        "\n",
        "(8 marks)"
      ],
      "metadata": {
        "id": "0x17i627INSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "query_tokens = word_tokenize(query.lower())\n",
        "query_tokens = [word for word in query_tokens if word.lower() not in stop and word.isalnum()]\n",
        "query_tf = {token: query_tokens.count(token) for token in set(query_tokens)}\n",
        "query_tfidf = {token: query_tf[token] * idf[token] for token in query_tf}\n",
        "\n",
        "similarities = []\n",
        "for i, tfidf in tfidf.items():\n",
        "    similarities.append(sim(query_tfidf, tfidf))\n",
        "\n",
        "sorted_similarities = sorted(similarities, reverse=True)\n",
        "for sim, i in zip(sorted_similarities, range(len(similarities))):\n",
        "    if sim > 0:\n",
        "        print(\"{}: {} ({})\".format(sim, sources[i], languages[i]))\n"
      ],
      "metadata": {
        "id": "2Mf_pHqq4kzk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1df712ac-80ed-412e-ba1f-019fd1a51d72"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3922322702763681: William Shakespeare (English)\n",
            "0.3592106040535498: John Kennedy (English)\n",
            "0.20203050891044214: the Bible (Greek)\n"
          ]
        }
      ]
    }
  ]
}