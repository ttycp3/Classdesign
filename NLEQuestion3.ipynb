{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ttycp3/Classdesign/blob/main/NLEQuestion3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#G5119: Natural Language Engineering\n",
        "\n",
        "##Computer Based Examination, 2023\n",
        "\n",
        "Remember, you can add cells and change their type (between code and text/markdown) as required to answer the questions."
      ],
      "metadata": {
        "id": "_M3r84BT5WeN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ibHDq-Im5Re4"
      },
      "outputs": [],
      "source": [
        "# update your candidate number here\n",
        "candidate_number = 625937"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#Question 3 (50 Marks)\n",
        "\n",
        "This question is about Named Entity Recognition"
      ],
      "metadata": {
        "id": "259FEqEb5xRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### do not change the code in this cell\n",
        "# make sure you run this cell\n",
        "import nltk\n",
        "\n",
        "# This is a list of sentences with NER tags which we will use in this question.\n",
        "tagged_sents=[[\"tim_PER\", \"cook_PER\", \"is\", \"the\", \"current\", \"ceo\", \"of\", \"apple_ORG\", \"inc._ORG\", \",\", \"headquartered\", \"in\", \"cupertino_LOC\", \".\"],\n",
        "              [\"fiona_PER\", \"apple_PER\", \"was\", \"born\", \"in\", \"new_LOC\", \"york_LOC\", \"and\", \"her\", \"debut\", \"album\", \"was\", \"released\", \"by\", \"columbia_ORG\", \"records_ORG\", \".\"],\n",
        "              [\"the_LOC\", \"big_LOC\", \"apple_LOC\", \"is\", \"a\", \"nickname\", \"for\", \"new_LOC\", \"york_LOC\", \"city_LOC\", \".\"]]\n"
      ],
      "metadata": {
        "id": "89Wx4WPa5weU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "a) Follow the steps below to preprocess the NER tagged sentences, producing a list of lists of tokens and a list of lists of NER tags."
      ],
      "metadata": {
        "id": "4MUMcHxbBsBi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "i) Use the `.split()` method to separate the tokens from the NER tags in the list `tagged_sents`, and strip off the tags to produce a list of lists of strings, called `tokens`, corresponding to just the tokens in each sentence.\n",
        "\n",
        "So, for example `[[\"Alice_PER\", \"runs\"], [\"Bob_PER\", \"walks\"]]` would produce `[[\"Alice\", \"runs\"], [\"Bob\", \"walks\"]]`.\n",
        "\n",
        "(8 marks)"
      ],
      "metadata": {
        "id": "9FDwzz0oCL8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokens = []\n",
        "for sent in tagged_sents:\n",
        "  token = []\n",
        "  for word in sent:\n",
        "    token.append(word.split('_')[0])\n",
        "  tokens.append(token)\n"
      ],
      "metadata": {
        "id": "9fY75wgToDMo"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ii) Use the `.split()` method to separate the tokens from the NER tags in the list `tagged_sents`, producing a list of lists of NER tags, called `tags`. Tokens without an NER tag should be tagged with the letter `\"O\"`.\n",
        "\n",
        "So, for example `[[\"Alice_PER\", \"runs\"], [\"chase\", \"Bob_PER\"]]` would produce `[[\"PER\", \"O\"], [\"O\", \"PER\"]]`.\n",
        "\n",
        "(6 marks)"
      ],
      "metadata": {
        "id": "glog6pY1NkcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tags = []\n",
        "for sent in tagged_sents:\n",
        "  tag = []\n",
        "  for word in sent:\n",
        "    tag.append(word.split('_')[1] if len(word.split('_')) > 1 else 'O')\n",
        "  tags.append(tag)\n"
      ],
      "metadata": {
        "id": "2ZgLxYBConpu"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) Now, follow the steps below to derive the transition and emission probabilities of a Hidden Markov Model for the NER tag sequences."
      ],
      "metadata": {
        "id": "MhzmSCQTRhJo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "i) Describe the two assumptions that a Hidden Markov Model for sequence tagging is based on.\n",
        "\n",
        "(4 marks)"
      ],
      "metadata": {
        "id": "8Lz2eIJ6Ry6w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Markov Property: The model assumes that the probability of a current state depends only on the previous state and not on any further past states, which is called the first-order Markov assumption. In other words, each element in the sequence is dependent solely on the immediately preceding element. Mathematically, this can be expressed as P(state(t) | state(1), ..., state(t-1)) = P(state(t) | state(t-1)), where 'state(t)' refers to the state at time 't'.\n",
        "\n",
        "2.Observation Independence: HMMs also assume that the observation (or output) at a given time step is conditionally independent of all other observations given the underlying hidden state at that time. This means that once you know the hidden state, the presence or absence of another observation does not affect the likelihood of the current observation. Formally, this can be written as P(observation(t) | observation(1), ..., observation(t-1), state(1), ..., state(t)) = P(observation(t) | state(t))."
      ],
      "metadata": {
        "id": "N2vWgcgIUnvp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ii) Create a dictionary, called `nercounts`, containing the total counts for each tag in the list `tags`. The keys of `nercounts` should be tags and the values should be the frequencies of those tags.\n",
        "\n",
        "(4 marks)"
      ],
      "metadata": {
        "id": "cDlCGNMdSHBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nercounts = dict()\n",
        "for sent in tags:\n",
        "  for tag in sent:\n",
        "    if tag not in nercounts:\n",
        "      nercounts[tag] = 0\n",
        "    nercounts[tag] += 1\n"
      ],
      "metadata": {
        "id": "ASbaMzO74wMg"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "iii) Using the lists `tags` and `tokens`, calculate the emission probabilities: $$p(token|tag)$$\n",
        "\n",
        "Put these probabilities in a dictionary of dictionaries, called `emission`, with outer keys being NER tags, inner keys being tokens and inner values being the emission probabilities.\n",
        "\n",
        "(8 marks)"
      ],
      "metadata": {
        "id": "SJ54_g706lpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emission = dict()\n",
        "for sent, tag in zip(tokens, tags):\n",
        "  for token, tag in zip(sent, tag):\n",
        "    if tag not in emission:\n",
        "      emission[tag] = dict()\n",
        "    if token not in emission[tag]:\n",
        "      emission[tag][token] = 0\n",
        "    emission[tag][token] += 1\n"
      ],
      "metadata": {
        "id": "Ttpe2z36R2n0"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "iv) Using the list `tags`, calculate the transition probabilities: $$p(tag_i|tag_{i-1})$$\n",
        "\n",
        "You will need to keep track of both the current tag, $tag_i$, and the previous tag, $tag_{i-1}$, and you will need to introduce a special tag, e.g. \"START\", for the previous tag at the beginning of a sequence.\n",
        "\n",
        "For example, the sequence of tags `[\"PER\", \"O\"]` would give rise to the following values of $i$, $tag_{i-1}$ and $tag_i$:\n",
        "\n",
        "|i|$tag_{i-1}$|$tag_i$|\n",
        "|---|---|---|\n",
        "|0|START|PER|\n",
        "|1|PER|O|\n",
        "\n",
        "Calculate the conditional probabilities of $tag_i$ given $tag_{i-1}$ and put the results in a dictionary of dictionaries, called `transition`, with outer keys being previous tags, inner keys being current tags and inner values being transition probabilities.\n",
        "\n",
        "(10 marks)"
      ],
      "metadata": {
        "id": "ugDBChyQ8MHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transition = dict()\n",
        "start_tag = 'START'\n",
        "for sent in tags:\n",
        "  sent = [start_tag] + sent\n",
        "  for i in range(len(sent)-1):\n",
        "    prev_tag = sent[i]\n",
        "    curr_tag = sent[i+1]\n",
        "    if prev_tag not in transition:\n",
        "      transition[prev_tag] = dict()\n",
        "    if curr_tag not in transition[prev_tag]:\n",
        "      transition[prev_tag][curr_tag] = 0\n",
        "    transition[prev_tag][curr_tag] += 1"
      ],
      "metadata": {
        "id": "bbRg_hyU_76c"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "v) Given a sequence of tokens, we want to find the most probable sequence of tags corresponding to those tokens. The brute force approach would simply evaluate the probability of every sequence of tags. Explain why we would want to avoid that approach and describe an alternative algorithm.\n",
        "\n",
        "(6 marks)"
      ],
      "metadata": {
        "id": "WDevraTjRH5X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Exponential Complexity: The number of possible tag sequences grows exponentially with the length of the input token sequence. For example, if there are n different tags and a sentence has m words, then there would be n^m possible tag combinations. This quickly becomes unmanageable even for relatively short sentences.\n",
        "\n",
        "2.Resource Intensive: Computing the probability for each sequence requires multiple multiplications, which is time-consuming and memory-intensive, especially when dealing with large vocabularies or complex models.\n",
        "\n",
        "3.Practical Limitations: Even with substantial computational resources, the time required to compute probabilities for all possible sequences could exceed any reasonable processing time limit.\n",
        "\n",
        "An alternative algorithm that avoids these issues is the Viterbi Algorithm. It is specifically designed to solve this problem efficiently by using dynamic programming. The Viterbi algorithm works as follows:\n",
        "\n",
        "1.It maintains a probability matrix where each cell (i, j) represents the maximum probability of observing the first i tokens with the last tag being j.\n",
        "\n",
        "2.At each step, it updates the probability matrix by considering the transition probabilities from previous tags to the current tag, along with the emission probabilities (the probability of the token given the tag).\n",
        "\n",
        "3.After traversing the entire input sequence, it backtracks through the probability matrix to find the single path with the highest overall probability, which corresponds to the most probable sequence of tags."
      ],
      "metadata": {
        "id": "F7YwQI7MZxEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "vi) If we are tagging a sequence and the current token is `\"apple\"` and the previous tag is `\"LOC\"`, then we want know to which tag in the current position  would maximise the probability of seeing that token.\n",
        "\n",
        "Define a function that takes a token, a previous tag, a dictionary of emission probabilities and a dictionary of transition probabilities and returns a dictionary of the probabilities $p(token, tag_i | tag_{i-1})$ for all values of $tag_i$.\n",
        "\n",
        "Apply this function to the case where the current token is `\"apple\"` and the previous tag is `\"LOC\"`.\n",
        "\n",
        "(4 marks)"
      ],
      "metadata": {
        "id": "vdxdMimsLwpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_probabilities(token, prev_tag, emission, transition):\n",
        "  probabilities = {}\n",
        "  for tag in emission:\n",
        "    if prev_tag in transition and tag in transition[prev_tag]:\n",
        "      transition_prob = transition[prev_tag][tag]\n",
        "    else:\n",
        "      transition_prob = 0\n",
        "    if tag in emission and token in emission[tag]:\n",
        "      emission_prob = emission[tag][token]\n",
        "    else:\n",
        "      emission_prob = 0\n",
        "    probabilities[tag] = transition_prob * emission_prob\n",
        "  return probabilities\n",
        "probabilities = get_probabilities(\"apple\", \"LOC\", emission, transition)\n",
        "print(probabilities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SkXcpnFvxq6",
        "outputId": "5c8f075e-2551-4176-d589-ba8ca8bd11d6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'PER': 0, 'O': 0, 'ORG': 0, 'LOC': 5}\n"
          ]
        }
      ]
    }
  ]
}